{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sdv.single_table import TVAESynthesizer\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.sampling import Condition\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\n",
    "    \"../../data/Angoss Knowledge Seeker - carclaims.txt/carclaims_original.csv\"\n",
    ")\n",
    "# Drop row with missing data\n",
    "df.drop(df[df[\"DayOfWeekClaimed\"] == \"0\"].index, inplace=True)\n",
    "# Drop ID column\n",
    "df.drop(columns=\"PolicyNumber\", inplace=True)\n",
    "\n",
    "# Train-test split\n",
    "carclaims_train, carclaims_test = train_test_split(df, test_size=0.2, random_state=141)\n",
    "\n",
    "# Load SDV metadata\n",
    "metadata = Metadata.load_from_json(filepath=\"carclaims_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "738"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carclaims_train['FraudFound'].value_counts()['Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space = [\n",
    "#     Integer(200, 2000, name=\"tvae_epochs\"),\n",
    "#     Integer(100, 500, name=\"tvae_batch_size\"),\n",
    "#     Integer(1, 6, name=\"tvae_compress_depth\"),\n",
    "#     Categorical([64, 128, 256, 512, 1024], name=\"tvae_compress_width\"),\n",
    "#     Integer(1, 6, name=\"tvae_decompress_depth\"),\n",
    "#     Categorical([64, 128, 256, 512, 1024], name=\"tvae_decompress_width\"),\n",
    "#     Categorical([64, 128, 256, 512, 1024], name=\"tvae_embedding_dim\"),\n",
    "#     Integer(64, 1024, name=\"rf_n_estimators\"),\n",
    "#     Categorical([\"gini\", \"entropy\", \"log_loss\"], name=\"rf_criterion\"),\n",
    "#     Integer(5, 100, name=\"rf_max_depth\"),\n",
    "#     Integer(2, 30, name=\"rf_min_samples_split\"),\n",
    "#     Integer(1, 30, name=\"rf_min_samples_leaf\"),\n",
    "# ]\n",
    "\n",
    "# @use_named_args(space)\n",
    "def train_and_predict(\n",
    "    tvae_epochs, # 300\n",
    "    # tvae_batch_size, # \n",
    "    tvae_compress_depth, # 2\n",
    "    tvae_compress_width, # 128\n",
    "    tvae_decompress_depth, # 2\n",
    "    tvae_decompress_width, # 128\n",
    "    tvae_embedding_dim, # 128\n",
    "    rf_n_estimators,\n",
    "    rf_criterion,\n",
    "    rf_max_depth,\n",
    "    rf_min_samples_split,\n",
    "    rf_min_samples_leaf,\n",
    "):\n",
    "    \n",
    "    print(f'Checking: tvae_epochs {tvae_epochs}, tvae_batch_size default, tvae_compress_depth {tvae_compress_depth}, tvae_compress_width {tvae_compress_width}, tvae_decompress_depth {tvae_decompress_depth}, tvae_decompress_width {tvae_decompress_width}, tvae_embedding_dim {tvae_embedding_dim}, rf_n_estimators {rf_n_estimators}, rf_criterion {rf_criterion}, rf_max_depth {rf_max_depth}, rf_min_samples_split {rf_min_samples_split}, rf_min_samples_leaf {rf_min_samples_leaf},')\n",
    "\n",
    "    tvae_compress_dims = [tvae_compress_width] * tvae_compress_depth\n",
    "    tvae_decompress_dims = [tvae_decompress_width] * tvae_decompress_depth\n",
    "    \n",
    "    print('Original dataset shape %s' % Counter(carclaims_train['FraudFound']))\n",
    "    rus = RandomUnderSampler(random_state=42)    \n",
    "    X_rus, y_rus = rus.fit_resample(carclaims_train.drop('FraudFound', axis=1), carclaims_train['FraudFound'])\n",
    "    \n",
    "    print('Undersampled dataset shape %s' % Counter(y_rus))\n",
    "    # Create synthesizer\n",
    "    synthesizer = TVAESynthesizer(\n",
    "        metadata, cuda=True,\n",
    "        epochs=tvae_epochs,  # 300\n",
    "        # batch_size=tvae_batch_size,  # Must be multiple of pac and / by 2 - 500\n",
    "        compress_dims=tvae_compress_dims,  # (256, 256)\n",
    "        embedding_dim=tvae_embedding_dim,  # 128\n",
    "        decompress_dims=tvae_decompress_dims,  # (256, 256),\n",
    "    )\n",
    "    synthesizer.fit(pd.concat([X_rus, y_rus], axis=1))\n",
    "    \n",
    "    major_cnt = carclaims_train['FraudFound'].value_counts()['No']\n",
    "    minor_cnt = carclaims_train['FraudFound'].value_counts()['Yes']\n",
    "    \n",
    "    balance_cnt = major_cnt - minor_cnt\n",
    "\n",
    "    # Conditions for balancing the data\n",
    "    fraud_samples = Condition(\n",
    "        # num_rows=20_000 + balance_cnt,\n",
    "        num_rows=balance_cnt,\n",
    "        column_values={'FraudFound': 'Yes'}\n",
    "    )\n",
    "\n",
    "    # non_fraud_samples = Condition(\n",
    "    #     num_rows=20_000,\n",
    "    #     column_values={'FraudFound': 'No'}\n",
    "    # )\n",
    "\n",
    "    # Create balanced synthetic data\n",
    "    synthetic_data = synthesizer.sample_from_conditions(\n",
    "        conditions=[fraud_samples, non_fraud_samples],\n",
    "        batch_size=1_000\n",
    "    )\n",
    "    \n",
    "    balanced_data = pd.concat([carclaims_train, synthetic_data], axis=0).reset_index(drop=True)\n",
    "    carclaims_test.reset_index(drop=True)\n",
    "    print('Balanced dataset shape %s' % Counter(balanced_data['FraudFound']))\n",
    "    \n",
    "    # X y split\n",
    "    X_train = balanced_data.drop('FraudFound', axis=1)\n",
    "    y_train = balanced_data['FraudFound']\n",
    "\n",
    "    X_test = carclaims_test.drop('FraudFound', axis=1)\n",
    "    y_test = carclaims_test['FraudFound']\n",
    "\n",
    "    # Encode target variable\n",
    "    y_train.loc[y_train[:] == 'No'] = 0\n",
    "    y_train.loc[y_train[:] == 'Yes'] = 1\n",
    "\n",
    "    y_test.loc[y_test[:] == 'No'] = 0\n",
    "    y_test.loc[y_test[:] == 'Yes'] = 1\n",
    "\n",
    "    y_train = y_train.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "\n",
    "    # Lebel Encode features\n",
    "    column_labels = {\n",
    "        'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "        'DayOfWeek': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "        'DayOfWeekClaimed': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "        'MonthClaimed': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "        'AgeOfPolicyHolder': ['16 to 17', '18 to 20', '21 to 25', '26 to 30', '31 to 35', '36 to 40', '41 to 50', '51 to 65', 'over 65'],\n",
    "        'NumberOfSuppliments': ['none', '1 to 2', '3 to 5', 'more than 5'],\n",
    "        'AddressChange-Claim': ['no change', 'under 6 months', '1 year', '2 to 3 years', '4 to 8 years'],\n",
    "        'NumberOfCars': ['1 vehicle', '2 vehicles', '3 to 4', '5 to 8', 'more than 8'],\n",
    "        'VehiclePrice': ['less than 20,000', '20,000 to 29,000', '30,000 to 39,000', '40,000 to 59,000', '60,000 to 69,000', 'more than 69,000'],\n",
    "        'Days:Policy-Accident': ['none', '1 to 7', '15 to 30', '8 to 15', 'more than 30'],\n",
    "        'Days:Policy-Claim': ['15 to 30', '8 to 15', 'more than 30'],\n",
    "        'PastNumberOfClaims': ['none', '1', '2 to 4', 'more than 4'],\n",
    "        'AgeOfVehicle': ['new', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', 'more than 7'],\n",
    "        'Make': ['Accura', 'BMW', 'Chevrolet', 'Dodge', 'Ferrari', 'Ford', 'Honda', 'Jaguar', 'Lexus', 'Mazda', 'Mecedes', 'Mercury', 'Nisson', 'Pontiac', 'Porche', 'Saab', 'Saturn', 'Toyota', 'VW']\n",
    "    \n",
    "        }\n",
    "\n",
    "    for column, labels in column_labels.items():\n",
    "        oe = OrdinalEncoder(categories=[labels], handle_unknown='error')\n",
    "        X_train[column] = oe.fit_transform(X_train[[column]])\n",
    "        X_test[column] = oe.transform(X_test[[column]])\n",
    "\n",
    "    # one hot encode\n",
    "    columns_one_hot = {\n",
    "        'AccidentArea': ['Rural', 'Urban'],\n",
    "        'Sex': ['Female', 'Male'],\n",
    "        'MaritalStatus': ['Divorced', 'Married', 'Single', 'Widow'],\n",
    "        'PoliceReportFiled': ['No', 'Yes'],\n",
    "        'WitnessPresent': ['No', 'Yes'],\n",
    "        'AgentType': ['External', 'Internal'],\n",
    "        'BasePolicy': ['All Perils', 'Collision', 'Liability'],\n",
    "        'Fault': ['Policy Holder', 'Third Party'],\n",
    "        'PolicyType': ['Sedan - All Perils', 'Sedan - Collision', 'Sedan - Liability','Sport - All Perils', 'Sport - Collision', 'Sport - Liability', 'Utility - All Perils', 'Utility - Collision', 'Utility - Liability'],\n",
    "        'VehicleCategory': ['Sedan', 'Sport', 'Utility'],\n",
    "        \n",
    "    }\n",
    "\n",
    "    for column, labels in columns_one_hot.items():\n",
    "        ohe = OneHotEncoder(sparse_output=False, categories=[labels], drop='first', handle_unknown='error')\n",
    "        encoded_nominal = ohe.fit_transform(X_train[[column]])\n",
    "        X_train = pd.concat([X_train, pd.DataFrame(encoded_nominal, columns=ohe.get_feature_names_out([column]), index=X_train.index)], axis=1)\n",
    "\n",
    "        encoded_nominal = ohe.transform(X_test[[column]])\n",
    "        X_test = pd.concat([X_test, pd.DataFrame(encoded_nominal, columns=ohe.get_feature_names_out([column]), index=X_test.index)], axis=1)\n",
    "\n",
    "    X_test.drop(columns=columns_one_hot.keys(), axis=1, inplace=True)\n",
    "    X_train.drop(columns=columns_one_hot.keys(), axis=1, inplace=True)\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=rf_n_estimators,\n",
    "        criterion=rf_criterion,\n",
    "        max_depth=rf_max_depth,\n",
    "        min_samples_split=rf_min_samples_split,\n",
    "        min_samples_leaf=rf_min_samples_leaf,\n",
    "        random_state=141,\n",
    "    )\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f} Precision: {precision:.4f} Recall: {recall:.4f} F1 Score: {f1:.4f}\")\n",
    "#     return -f1\n",
    "# train_and_predict()\n",
    "    \n",
    "# res_gp = gp_minimize(train_and_predict, space, n_calls=50, random_state=141)\n",
    "\n",
    "# print(f'Best f1: {res_gp.fun}')\n",
    "# print(f'Best params: {res_gp.x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20000 epochs + OrdinalEncoder\n",
    "\n",
    "Accuracy: 0.7455 Precision: 0.1411 Recall: 0.6378 F1 Score: 0.2311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2000 epochs + OrdinalEncoder\n",
    "\n",
    "Accuracy: 0.8100 Precision: 0.1607 Recall: 0.5135 F1 Score: 0.2448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking: tvae_epochs 30000, tvae_batch_size default, tvae_compress_depth 5, tvae_compress_width 512, tvae_decompress_depth 5, tvae_decompress_width 512, tvae_embedding_dim 512, rf_n_estimators 115, rf_criterion gini, rf_max_depth 10, rf_min_samples_split 30, rf_min_samples_leaf 16,\n",
      "Original dataset shape Counter({'No': 11597, 'Yes': 738})\n",
      "Undersampled dataset shape Counter({'No': 738, 'Yes': 738})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 51000it [00:12, 4123.75it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset shape Counter({'No': 31597, 'Yes': 31597})\n",
      "Accuracy: 0.6530 Precision: 0.1352 Recall: 0.8865 F1 Score: 0.2346\n"
     ]
    }
   ],
   "source": [
    "train_and_predict(\n",
    "    tvae_epochs=30000, # 300\n",
    "    tvae_compress_depth=5, # 2\n",
    "    tvae_compress_width=512, # 128\n",
    "    tvae_decompress_depth=5, # 2\n",
    "    tvae_decompress_width=512, # 128\n",
    "    tvae_embedding_dim=512, # 128\n",
    "    rf_n_estimators=115,\n",
    "    rf_criterion='gini',\n",
    "    rf_max_depth=10,\n",
    "    rf_min_samples_split=30,\n",
    "    rf_min_samples_leaf=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking: tvae_epochs 30000, tvae_batch_size default, tvae_compress_depth 5, tvae_compress_width 512, tvae_decompress_depth 5, tvae_decompress_width 512, tvae_embedding_dim 512, rf_n_estimators 115, rf_criterion gini, rf_max_depth 10, rf_min_samples_split 30, rf_min_samples_leaf 16,\n",
    "\n",
    "Original dataset shape Counter({'No': 11597, 'Yes': 738})\n",
    "\n",
    "Undersampled dataset shape Counter({'No': 738, 'Yes': 738})\n",
    "\n",
    "Sampling conditions: : 51000it [00:12, 4123.75it/s]                         \n",
    "\n",
    "Balanced dataset shape Counter({'No': 31597, 'Yes': 31597})\n",
    "\n",
    "Accuracy: 0.6530 Precision: 0.1352 Recall: 0.8865 F1 Score: 0.2346"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlr503-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
