{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sdv.single_table import TVAESynthesizer\n",
    "from sdv.metadata import Metadata\n",
    "from collections import Counter\n",
    "from sdv.evaluation.single_table import evaluate_quality\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sdv.sampling import Condition\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\n",
    "    \"../../data/Angoss Knowledge Seeker - carclaims.txt/carclaims_original.csv\"\n",
    ")\n",
    "# Drop row with missing data\n",
    "df.drop(df[df[\"DayOfWeekClaimed\"] == \"0\"].index, inplace=True)\n",
    "# Drop ID column\n",
    "df.drop(columns=\"PolicyNumber\", inplace=True)\n",
    "\n",
    "# Train-test split\n",
    "carclaims_train, carclaims_test = train_test_split(df, test_size=0.2, random_state=141)\n",
    "\n",
    "# Load SDV metadata\n",
    "metadata = Metadata.load_from_json(filepath=\"carclaims_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({'No': 11597, 'Yes': 738})\n",
      "Undersampled dataset shape Counter({'No': 738, 'Yes': 738})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling rows: 100%|██████████| 10000/10000 [00:00<00:00, 12155.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 32/32 [00:00<00:00, 329.17it/s]|\n",
      "Column Shapes Score: 92.46%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 496/496 [00:02<00:00, 211.48it/s]|\n",
      "Column Pair Trends Score: 87.03%\n",
      "\n",
      "Overall Score (Average): 89.75%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset shape %s' % Counter(carclaims_train['FraudFound']))\n",
    "rus = RandomUnderSampler(random_state=42)    \n",
    "X_rus, y_rus = rus.fit_resample(carclaims_train.drop('FraudFound', axis=1), carclaims_train['FraudFound'])\n",
    "\n",
    "print('Undersampled dataset shape %s' % Counter(y_rus))\n",
    "# Create synthesizer\n",
    "synthesizer = TVAESynthesizer(\n",
    "    metadata, cuda=True,\n",
    "    epochs=30000,  # 300\n",
    "    compress_dims=[256, 256],  # (256, 256)\n",
    "    decompress_dims=[256, 256],  # (256, 256),\n",
    "    embedding_dim=512,  # 512\n",
    ")\n",
    "synthesizer.fit(pd.concat([X_rus, y_rus], axis=1))\n",
    "\n",
    "synthesizer.save(filepath='my_synthesizer_3.pkl')\n",
    "\n",
    "synthetic_data = synthesizer.sample(\n",
    "    num_rows=10_000,\n",
    "    batch_size=1_000\n",
    ")\n",
    "\n",
    "quality_report = evaluate_quality(\n",
    "    real_data=df,\n",
    "    synthetic_data=synthetic_data,\n",
    "    metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20000 epochs + OrdinalEncoder\n",
    "\n",
    "Accuracy: 0.7455 Precision: 0.1411 Recall: 0.6378 F1 Score: 0.2311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2000 epochs + OrdinalEncoder\n",
    "\n",
    "Accuracy: 0.8100 Precision: 0.1607 Recall: 0.5135 F1 Score: 0.2448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking: tvae_epochs 20000, tvae_batch_size default, tvae_compress_depth 5, tvae_compress_width 64, tvae_decompress_depth 5, tvae_decompress_width 512, tvae_embedding_dim 32, rf_n_estimators 500, rf_criterion gini, rf_max_depth 21, rf_min_samples_split 13, rf_min_samples_leaf 2,\n",
      "Original dataset shape Counter({'No': 11597, 'Yes': 738})\n",
      "Undersampled dataset shape Counter({'No': 738, 'Yes': 738})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 11000it [00:01, 5566.21it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset shape Counter({'No': 11597, 'Yes': 11597})\n",
      "Accuracy: 0.9176 Precision: 0.1584 Recall: 0.0865 F1 Score: 0.1119\n"
     ]
    }
   ],
   "source": [
    "train_and_predict(\n",
    "    tvae_epochs=20000, # 300\n",
    "    tvae_compress_depth=5, # 2\n",
    "    tvae_compress_width=64, # 128\n",
    "    tvae_decompress_depth=5, # 2\n",
    "    tvae_decompress_width=512, # 128\n",
    "    tvae_embedding_dim=32, # 128\n",
    "    rf_n_estimators=500,\n",
    "    rf_criterion='gini',\n",
    "    rf_max_depth=21,\n",
    "    rf_min_samples_split=13,\n",
    "    rf_min_samples_leaf=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking: tvae_epochs 30000, tvae_batch_size default, tvae_compress_depth 5, tvae_compress_width 512, tvae_decompress_depth 5, tvae_decompress_width 512, tvae_embedding_dim 512, rf_n_estimators 115, rf_criterion gini, rf_max_depth 10, rf_min_samples_split 30, rf_min_samples_leaf 16,\n",
    "\n",
    "Original dataset shape Counter({'No': 11597, 'Yes': 738})\n",
    "\n",
    "Undersampled dataset shape Counter({'No': 738, 'Yes': 738})\n",
    "\n",
    "Sampling conditions: : 51000it [00:12, 4123.75it/s]                         \n",
    "\n",
    "Balanced dataset shape Counter({'No': 31597, 'Yes': 31597})\n",
    "\n",
    "Accuracy: 0.6530 Precision: 0.1352 Recall: 0.8865 F1 Score: 0.2346"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking: tvae_epochs 30000, tvae_batch_size default, tvae_compress_depth 5, tvae_compress_width 512, tvae_decompress_depth 5, tvae_decompress_width 512, tvae_embedding_dim 512, rf_n_estimators 115, rf_criterion gini, rf_max_depth 10, rf_min_samples_split 30, rf_min_samples_leaf 16,\n",
    "\n",
    "Original dataset shape Counter({'No': 11597, 'Yes': 738})\n",
    "\n",
    "Undersampled dataset shape Counter({'No': 738, 'Yes': 738})\n",
    "\n",
    "Sampling conditions: : 11000it [00:02, 4874.63it/s]                         \n",
    "\n",
    "Balanced dataset shape Counter({'No': 11597, 'Yes': 11597})\n",
    "\n",
    "Accuracy: 0.6702 Precision: 0.1408 Recall: 0.8811 F1 Score: 0.2427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'enforce_min_max_values': True, 'enforce_rounding': True, 'embedding_dim': 512, 'compress_dims': [256, 256, 256, 256, 256], 'decompress_dims': [1024, 1024, 1024, 1024, 1024], 'l2scale': 1e-05, 'batch_size': 500, 'verbose': False, 'epochs': 20000, 'loss_factor': 2, 'cuda': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 11000it [00:05, 2086.90it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset shape Counter({'No': 11597, 'Yes': 11597})\n"
     ]
    }
   ],
   "source": [
    "synthesizer = TVAESynthesizer.load(\n",
    "    filepath='my_synthesizer_92_8.pkl'\n",
    ")\n",
    "\n",
    "print(synthesizer.get_parameters())\n",
    "major_cnt = carclaims_train['FraudFound'].value_counts()['No']\n",
    "minor_cnt = carclaims_train['FraudFound'].value_counts()['Yes']\n",
    "\n",
    "balance_cnt = major_cnt - minor_cnt\n",
    "# Conditions for balancing the data\n",
    "fraud_samples = Condition(\n",
    "    # num_rows=20_000 + balance_cnt,\n",
    "    num_rows=balance_cnt,\n",
    "    column_values={'FraudFound': 'Yes'}\n",
    ")\n",
    "# non_fraud_samples = Condition(\n",
    "#     num_rows=20_000,\n",
    "#     column_values={'FraudFound': 'No'}\n",
    "# )\n",
    "# Create balanced synthetic data\n",
    "synthetic_data = synthesizer.sample_from_conditions(\n",
    "    conditions=[fraud_samples],\n",
    "    # conditions=[fraud_samples, non_fraud_samples],\n",
    "    batch_size=1_000\n",
    ")\n",
    "\n",
    "balanced_data = pd.concat([carclaims_train, synthetic_data], axis=0).reset_index(drop=True)\n",
    "carclaims_test.reset_index(drop=True)\n",
    "print('Balanced dataset shape %s' % Counter(balanced_data['FraudFound']))\n",
    "\n",
    "# X y split\n",
    "X_train = balanced_data.drop('FraudFound', axis=1)\n",
    "y_train = balanced_data['FraudFound']\n",
    "X_test = carclaims_test.drop('FraudFound', axis=1)\n",
    "y_test = carclaims_test['FraudFound']\n",
    "# Encode target variable\n",
    "y_train = y_train.map({'Yes': 1, 'No': 0})\n",
    "y_test = y_test.map({'Yes': 1, 'No': 0})\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "# Lebel Encode features\n",
    "column_labels = {\n",
    "    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "    'DayOfWeek': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "    'DayOfWeekClaimed': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "    'MonthClaimed': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "    'AgeOfPolicyHolder': ['16 to 17', '18 to 20', '21 to 25', '26 to 30', '31 to 35', '36 to 40', '41 to 50', '51 to 65', 'over 65'],\n",
    "    'NumberOfSuppliments': ['none', '1 to 2', '3 to 5', 'more than 5'],\n",
    "    'AddressChange-Claim': ['no change', 'under 6 months', '1 year', '2 to 3 years', '4 to 8 years'],\n",
    "    'NumberOfCars': ['1 vehicle', '2 vehicles', '3 to 4', '5 to 8', 'more than 8'],\n",
    "    'VehiclePrice': ['less than 20,000', '20,000 to 29,000', '30,000 to 39,000', '40,000 to 59,000', '60,000 to 69,000', 'more than 69,000'],\n",
    "    'Days:Policy-Accident': ['none', '1 to 7', '15 to 30', '8 to 15', 'more than 30'],\n",
    "    'Days:Policy-Claim': ['15 to 30', '8 to 15', 'more than 30'],\n",
    "    'PastNumberOfClaims': ['none', '1', '2 to 4', 'more than 4'],\n",
    "    'AgeOfVehicle': ['new', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', 'more than 7'],\n",
    "    'Make': ['Accura', 'BMW', 'Chevrolet', 'Dodge', 'Ferrari', 'Ford', 'Honda', 'Jaguar', 'Lexus', 'Mazda', 'Mecedes', 'Mercury', 'Nisson', 'Pontiac', 'Porche', 'Saab', 'Saturn', 'Toyota', 'VW']\n",
    "\n",
    "    }\n",
    "for column, labels in column_labels.items():\n",
    "    oe = OrdinalEncoder(categories=[labels], handle_unknown='error')\n",
    "    X_train[column] = oe.fit_transform(X_train[[column]])\n",
    "    X_test[column] = oe.transform(X_test[[column]])\n",
    "# one hot encode\n",
    "columns_one_hot = {\n",
    "    'AccidentArea': ['Rural', 'Urban'],\n",
    "    'Sex': ['Female', 'Male'],\n",
    "    'MaritalStatus': ['Divorced', 'Married', 'Single', 'Widow'],\n",
    "    'PoliceReportFiled': ['No', 'Yes'],\n",
    "    'WitnessPresent': ['No', 'Yes'],\n",
    "    'AgentType': ['External', 'Internal'],\n",
    "    'BasePolicy': ['All Perils', 'Collision', 'Liability'],\n",
    "    'Fault': ['Policy Holder', 'Third Party'],\n",
    "    'PolicyType': ['Sedan - All Perils', 'Sedan - Collision', 'Sedan - Liability','Sport - All Perils', 'Sport - Collision', 'Sport - Liability', 'Utility - All Perils', 'Utility - Collision', 'Utility - Liability'],\n",
    "    'VehicleCategory': ['Sedan', 'Sport', 'Utility'],\n",
    "    \n",
    "}\n",
    "for column, labels in columns_one_hot.items():\n",
    "    ohe = OneHotEncoder(sparse_output=False, categories=[labels], drop='first', handle_unknown='error')\n",
    "    encoded_nominal = ohe.fit_transform(X_train[[column]])\n",
    "    X_train = pd.concat([X_train, pd.DataFrame(encoded_nominal, columns=ohe.get_feature_names_out([column]), index=X_train.index)], axis=1)\n",
    "    encoded_nominal = ohe.transform(X_test[[column]])\n",
    "    X_test = pd.concat([X_test, pd.DataFrame(encoded_nominal, columns=ohe.get_feature_names_out([column]), index=X_test.index)], axis=1)\n",
    "X_test.drop(columns=columns_one_hot.keys(), axis=1, inplace=True)\n",
    "X_train.drop(columns=columns_one_hot.keys(), axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8797 Precision: 0.2057 Recall: 0.3514 F1 Score: 0.2595\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    # criterion='entropy',\n",
    "    # max_depth=32,\n",
    "    # min_samples_split=rf_min_samples_split,\n",
    "    # min_samples_leaf=rf_min_samples_leaf,\n",
    "    random_state=42,\n",
    ")\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f} Precision: {precision:.4f} Recall: {recall:.4f} F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "synthesizer = TVAESynthesizer(\n",
    "    metadata, cuda=True,\n",
    "    epochs=20000,  # 300\n",
    "    compress_dims=[1024, 512, 256, 64],  # (256, 256)\n",
    "    decompress_dims=[64, 256, 512, 1024],  # (256, 256),\n",
    "    embedding_dim=64,  # 128\n",
    ")\n",
    "synthesizer.fit(pd.concat([X_rus, y_rus], axis=1))\n",
    "\n",
    "synthesizer.save(filepath='my_synthesizer_2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "256 256 256, 1024 1024 1024 Overall Score (Average): 90.94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:48:02] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:48:02] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:48:02] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:48:02] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:48:02] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:48:29] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:48:30] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:48:30] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:48:30] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:48:30] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:51:16] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:51:16] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:51:16] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:51:17] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [22:51:17] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [23:01:44] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [23:01:44] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [23:01:45] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [23:01:45] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/abdussaboor/.virtualenvs/mlr503-project/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [23:01:45] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"gamma\", \"max_depth\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val. score: 0.3000105922468458\n",
      "test score: 0.28703703703703703\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# xgb = XGBClassifier(\n",
    "#     # n_estimators=2, \n",
    "#     # max_depth=10, \n",
    "#     # learning_rate=1, \n",
    "#     # objective=\"binary:logistic\"\n",
    "# )\n",
    "def scorer(estimator, X, y):\n",
    "    return f1_score(y_test, estimator.predict(X_test))\n",
    "    \n",
    "opt = BayesSearchCV(\n",
    "    XGBClassifier(),\n",
    "    {\n",
    "        'n_estimators': (2, 200),\n",
    "        'max_depth': (2, 20),\n",
    "        'gamma': (1e-6, 1e+1, 'log-uniform'),\n",
    "        'reg_lambda': (1, 8, 'uniform'), \n",
    "        # 'sampling_method': ['uniform', 'gradient_based'],\n",
    "        'booster': ['gbtree'],\n",
    "        'learning_rate': (0.001, 1, 'uniform'),\n",
    "        'subsample': (0.5, 1, 'uniform')\n",
    "    },\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    scoring=scorer\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "print(\"val. score: %s\" % opt.best_score_)\n",
    "print(\"test score: %s\" % opt.score(X_test, y_test))\n",
    "# xgb.fit(X_train, y_train)\n",
    "# y_pred = xgb.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy:.4f} Precision: {precision:.4f} Recall: {recall:.4f} F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('booster', 'gbtree'),\n",
       "             ('gamma', 6.3588020322349665),\n",
       "             ('max_depth', 35),\n",
       "             ('n_estimators', 199),\n",
       "             ('reg_lambda', 8),\n",
       "             ('tree_method', 'exact')])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8979 Precision: 0.2441 Recall: 0.3351 F1 Score: 0.2825\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# xgb = XGBClassifier(\n",
    "#     n_estimators=15, \n",
    "#     gamma=2e-6,\n",
    "#     max_depth=33, \n",
    "#     # learning_rate=1,\n",
    "#     booster='gbtree',\n",
    "#     reg_lambda=8,\n",
    "#     objective=\"binary:logistic\"\n",
    "# )\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=199, \n",
    "    gamma=2e-6,\n",
    "    max_depth=35, \n",
    "    # learning_rate=1,\n",
    "    booster='gbtree',\n",
    "    reg_lambda=8,\n",
    "    tree_method='exact'\n",
    "    # objective=\"binary:logistic\"\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f} Precision: {precision:.4f} Recall: {recall:.4f} F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling rows:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling rows: 100%|██████████| 10000/10000 [00:00<00:00, 12976.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 32/32 [00:00<00:00, 332.09it/s]|\n",
      "Column Shapes Score: 92.46%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 496/496 [00:02<00:00, 208.76it/s]|\n",
      "Column Pair Trends Score: 87.03%\n",
      "\n",
      "Overall Score (Average): 89.75%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "synthesizer = TVAESynthesizer.load(\n",
    "    filepath='my_synthesizer_3.pkl'\n",
    ")\n",
    "\n",
    "synthetic_data = synthesizer.sample(\n",
    "    num_rows=10_000,\n",
    "    batch_size=1_000\n",
    ")\n",
    "\n",
    "quality_report = evaluate_quality(\n",
    "    real_data=df,\n",
    "    synthetic_data=synthetic_data,\n",
    "    metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlr503-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
