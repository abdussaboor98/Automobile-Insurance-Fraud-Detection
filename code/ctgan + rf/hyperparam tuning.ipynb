{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.sampling import Condition\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\n",
    "    \"../../data/Angoss Knowledge Seeker - carclaims.txt/carclaims_original.csv\"\n",
    ")\n",
    "# Drop row with missing data\n",
    "df.drop(df[df[\"DayOfWeekClaimed\"] == \"0\"].index, inplace=True)\n",
    "# Drop ID column\n",
    "df.drop(columns=\"PolicyNumber\", inplace=True)\n",
    "\n",
    "# Train-test split\n",
    "carclaims_train, carclaims_test = train_test_split(df, test_size=0.2, random_state=141)\n",
    "\n",
    "# Load SDV metadata\n",
    "metadata = Metadata.load_from_json(filepath=\"carclaims_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "space = [\n",
    "    Integer(200, 1000, name=\"ctgan_epochs\"),\n",
    "    Integer(5, 100, name=\"ctgan_pac\"),\n",
    "    Integer(50, 100, name=\"ctgan_batch_multiple\"),\n",
    "    Real(1e-8, 1e2, name=\"ctgan_discriminator_decay\"),\n",
    "    Real(2e-6, 2e2, \"log-uniform\", name=\"ctgan_discriminator_lr\"),\n",
    "    Real(1e-8, 1e2, name=\"ctgan_generator_decay\"),\n",
    "    Real(2e-6, 2e2, \"log-uniform\", name=\"ctgan_generator_lr\"),\n",
    "    Integer(1, 6, name=\"ctgan_discriminator_layers\"),\n",
    "    Categorical([64, 128, 256, 512, 1024], name=\"ctgan_discriminator_width\"),\n",
    "    Integer(1, 6, name=\"ctgan_generator_layers\"),\n",
    "    Categorical([64, 128, 256, 512, 1024], name=\"ctgan_generator_width\"),\n",
    "    Categorical([64, 128, 256, 512, 1024], name=\"ctgan_embedding_dim\"),\n",
    "    Integer(64, 1024, name=\"rf_n_estimators\"),\n",
    "    Categorical([\"gini\", \"entropy\", \"log_loss\"], name=\"rf_criterion\"),\n",
    "    Integer(5, 50, name=\"rf_max_depth\"),\n",
    "    Integer(2, 50, name=\"rf_min_samples_split\"),\n",
    "    Integer(1, 50, name=\"rf_min_samples_leaf\"),\n",
    "]\n",
    "\n",
    "@use_named_args(space)\n",
    "def train_and_predict(\n",
    "    ctgan_epochs, # 300\n",
    "    ctgan_pac, # 10\n",
    "    ctgan_batch_multiple, # 50\n",
    "    ctgan_discriminator_decay, # 1e-6\n",
    "    ctgan_discriminator_lr, #2e-4\n",
    "    ctgan_generator_decay,\n",
    "    ctgan_generator_lr,\n",
    "    ctgan_discriminator_layers, # 2\n",
    "    ctgan_discriminator_width, # 256\n",
    "    ctgan_generator_layers, # 2\n",
    "    ctgan_generator_width, # 256\n",
    "    ctgan_embedding_dim, # 128\n",
    "    rf_n_estimators,\n",
    "    rf_criterion,\n",
    "    rf_max_depth,\n",
    "    rf_min_samples_split,\n",
    "    rf_min_samples_leaf,\n",
    "    ctgan_discriminator_steps = 1, # 1\n",
    "):\n",
    "\n",
    "    ctgan_batch_size = ctgan_pac * ctgan_batch_multiple\n",
    "    if ctgan_batch_size % 2 != 0:\n",
    "        ctgan_batch_size += 1\n",
    "    ctgan_discriminator_dim = [ctgan_discriminator_width] * ctgan_discriminator_layers\n",
    "    ctgan_generator_dim = [ctgan_generator_width] * ctgan_generator_layers\n",
    "\n",
    "    # Create synthesizer\n",
    "    synthesizer = CTGANSynthesizer(\n",
    "        metadata,\n",
    "        epochs=ctgan_epochs,  # 300\n",
    "        batch_size=ctgan_batch_size,  # Must be multiple of pac and / by 2 - 500\n",
    "        pac=ctgan_pac,  # 10\n",
    "        discriminator_decay=ctgan_discriminator_decay,  # 1e-6\n",
    "        generator_decay=ctgan_generator_decay,  # 1e-6\n",
    "        discriminator_lr=ctgan_discriminator_lr,  # 2e-4\n",
    "        generator_lr=ctgan_generator_lr,  # 2e-4\n",
    "        discriminator_steps=ctgan_discriminator_steps,  # 1 (As per original CTGAN)\n",
    "        discriminator_dim=ctgan_discriminator_dim,  # (256, 256)\n",
    "        embedding_dim=ctgan_embedding_dim,  # 128\n",
    "        generator_dim=ctgan_generator_dim,  # (256, 256)\n",
    "    )\n",
    "    synthesizer.fit(df)\n",
    "\n",
    "    # Conditions for balancing the data\n",
    "    fraud_samples = Condition(\n",
    "        num_rows=50_000,\n",
    "        column_values={'FraudFound': 'Yes'}\n",
    "    )\n",
    "\n",
    "    non_fraud_samples = Condition(\n",
    "        num_rows=50_000,\n",
    "        column_values={'FraudFound': 'No'}\n",
    "    )\n",
    "\n",
    "    # Create balanced synthetic data\n",
    "    synthetic_data = synthesizer.sample_from_conditions(\n",
    "        conditions=[fraud_samples, non_fraud_samples],\n",
    "        batch_size=1_000\n",
    "    )\n",
    "\n",
    "    # X y split\n",
    "    X_train = synthetic_data.drop('FraudFound', axis=1).reset_index(drop=True)\n",
    "    y_train = synthetic_data['FraudFound'].reset_index(drop=True)\n",
    "\n",
    "    X_test = carclaims_test.drop('FraudFound', axis=1).reset_index(drop=True)\n",
    "    y_test = carclaims_test['FraudFound'].reset_index(drop=True)\n",
    "\n",
    "    # Encode target variable\n",
    "    y_train.loc[y_train[:] == 'No'] = 0\n",
    "    y_train.loc[y_train[:] == 'Yes'] = 1\n",
    "\n",
    "    y_test.loc[y_test[:] == 'No'] = 0\n",
    "    y_test.loc[y_test[:] == 'Yes'] = 1\n",
    "\n",
    "    y_train = y_train.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "\n",
    "    # Lebel Encode features\n",
    "    column_labels = {\n",
    "        'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "        'DayOfWeek': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "        'DayOfWeekClaimed': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "        'MonthClaimed': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "        'AgeOfPolicyHolder': ['16 to 17', '18 to 20', '21 to 25', '26 to 30', '31 to 35', '36 to 40', '41 to 50', '51 to 65', 'over 65'],\n",
    "        'NumberOfSuppliments': ['none', '1 to 2', '3 to 5', 'more than 5'],\n",
    "        'AddressChange-Claim': ['no change', 'under 6 months', '1 year', '2 to 3 years', '4 to 8 years'],\n",
    "        'NumberOfCars': ['1 vehicle', '2 vehicles', '3 to 4', '5 to 8', 'more than 8'],\n",
    "        'VehiclePrice': ['less than 20,000', '20,000 to 29,000', '30,000 to 39,000', '40,000 to 59,000', '60,000 to 69,000', 'more than 69,000'],\n",
    "        'Days:Policy-Accident': ['none', '1 to 7', '15 to 30', '8 to 15', 'more than 30'],\n",
    "        'Days:Policy-Claim': ['15 to 30', '8 to 15', 'more than 30'],\n",
    "        'PastNumberOfClaims': ['none', '1', '2 to 4', 'more than 4'],\n",
    "        'AgeOfVehicle': ['new', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', 'more than 7'],\n",
    "        'Make': ['Accura', 'BMW', 'Chevrolet', 'Dodge', 'Ferrari', 'Ford', 'Honda', 'Jaguar', 'Lexus', 'Mazda', 'Mecedes', 'Mercury', 'Nisson', 'Pontiac', 'Porche', 'Saab', 'Saturn', 'Toyota', 'VW']\n",
    "    }\n",
    "\n",
    "    for column, labels in column_labels.items():\n",
    "        oe = OrdinalEncoder(categories=[labels], handle_unknown='error')\n",
    "        X_train[column] = oe.fit_transform(X_train[column].values.reshape(-1, 1))\n",
    "        X_test[column] = oe.transform(X_test[column].values.reshape(-1, 1))\n",
    "\n",
    "    # one hot encode\n",
    "    columns_one_hot = {\n",
    "        'AccidentArea': ['Rural', 'Urban'],\n",
    "        'Sex': ['Female', 'Male'],\n",
    "        'MaritalStatus': ['Divorced', 'Married', 'Single', 'Widow'],\n",
    "        'PoliceReportFiled': ['No', 'Yes'],\n",
    "        'WitnessPresent': ['No', 'Yes'],\n",
    "        'AgentType': ['External', 'Internal'],\n",
    "        'BasePolicy': ['All Perils', 'Collision', 'Liability'],\n",
    "        'Fault': ['Policy Holder', 'Third Party'],\n",
    "        'PolicyType': ['Sedan - All Perils', 'Sedan - Collision', 'Sedan - Liability','Sport - All Perils', 'Sport - Collision', 'Sport - Liability', 'Utility - All Perils', 'Utility - Collision', 'Utility - Liability'],\n",
    "        'VehicleCategory': ['Sedan', 'Sport', 'Utility']\n",
    "    }\n",
    "\n",
    "    for column, labels in columns_one_hot.items():\n",
    "        ohe = OneHotEncoder(sparse_output=False, categories=[labels], drop='first', handle_unknown='error')\n",
    "        encoded_nominal = ohe.fit_transform(X_train[[column]])\n",
    "        X_train = pd.concat([X_train, pd.DataFrame(encoded_nominal, columns=ohe.get_feature_names_out([column]))], axis=1)\n",
    "\n",
    "        encoded_nominal = ohe.transform(X_test[[column]])\n",
    "        X_test = pd.concat([X_test, pd.DataFrame(encoded_nominal, columns=ohe.get_feature_names_out([column]))], axis=1)\n",
    "\n",
    "    X_test.drop(columns=columns_one_hot.keys(), axis=1, inplace=True)\n",
    "    X_train.drop(columns=columns_one_hot.keys(), axis=1, inplace=True)\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=rf_n_estimators,\n",
    "        criterion=rf_criterion,\n",
    "        max_depth=rf_max_depth,\n",
    "        min_samples_split=rf_min_samples_split,\n",
    "        min_samples_leaf=rf_min_samples_leaf,\n",
    "        random_state=141,\n",
    "    )\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # accuracy = accuracy_score(y_test, y_pred)\n",
    "    # precision = precision_score(y_test, y_pred)\n",
    "    # recall = recall_score(y_test, y_pred)\n",
    "    return -f1_score(y_test, y_pred)\n",
    "\n",
    "    # print(\"\\n=== XGBoost on SMOTE + Autoencoder Processed Data ===\")\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"Precision: {precision:.4f}\")\n",
    "    # print(f\"Recall: {recall:.4f}\")\n",
    "    # print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "res_gp = gp_minimize(train_and_predict, space, n_calls=50, random_state=141)\n",
    "\n",
    "print(f'Best f1: {res_gp.fun}')\n",
    "print(f'Best params: {res_gp.x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[126, 126, 126]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlr503-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
