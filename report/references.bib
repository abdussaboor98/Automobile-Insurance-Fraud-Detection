@article{viaeneInsuranceFraudIssues2004,
  title = {Insurance {{Fraud}}: {{Issues}} and {{Challenges}}},
  shorttitle = {Insurance {{Fraud}}},
  author = {Viaene, Stijn and Dedene, Guido},
  year = {2004},
  month = apr,
  journal = {The Geneva Papers on Risk and Insurance - Issues and Practice},
  volume = {29},
  number = {2},
  pages = {313--333},
  issn = {1468-0440},
  doi = {10.1111/j.1468-0440.2004.00290.x},
  urldate = {2024-12-05},
  abstract = {This article is devoted to the phenomenon of insurance fraud. We start by surveying the various forms of insurance fraud as well as its extent and cost. We proceed to analyse the problem as the product of motivation and opportunity, and address the complexities of fraud control. Finally, we provide a high-level overview of current anti-fraud activity.},
  langid = {english},
  file = {C:\Users\abdusssboor\Zotero\storage\E37W3EX4\Viaene and Dedene - 2004 - Insurance Fraud Issues and Challenges.pdf}
}
@article{phuaComprehensiveSurveyData2012,
  title = {A {{Comprehensive Survey}} of {{Data Mining-based Fraud Detection Research}}},
  author = {Phua, Clifton and Lee, Vincent and Smith, Kate and Gayler, Ross},
  year = {2012},
  month = may,
  journal = {Computers in Human Behavior},
  volume = {28},
  number = {3},
  eprint = {1009.6119},
  primaryclass = {cs},
  pages = {1002--1013},
  issn = {07475632},
  doi = {10.1016/j.chb.2012.01.002},
  urldate = {2024-12-06},
  abstract = {This survey paper categorises, compares, and summarises from almost all published technical and review articles in automated fraud detection within the last 10 years. It defines the professional fraudster, formalises the main types and subtypes of known fraud, and presents the nature of data evidence collected within affected industries. Within the business context of mining the data to achieve higher cost savings, this research presents methods and techniques together with their problems. Compared to all related reviews on fraud detection, this survey covers much more technical articles and is the only one, to the best of our knowledge, which proposes alternative data and solutions from related domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Engineering Finance and Science},
  file = {C\:\\Users\\abdusssboor\\Zotero\\storage\\4QVAAAM7\\Phua et al. - 2012 - A Comprehensive Survey of Data Mining-based Fraud Detection Research.pdf;C\:\\Users\\abdusssboor\\Zotero\\storage\\5R53FCPF\\1009.html}
}

@article{Kaufman2012,
author = {Kaufman, Shachar and Rosset, Saharon and Perlich, Claudia and Stitelman, Ori},
title = {Leakage in data mining: Formulation, detection, and avoidance},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2382577.2382579},
doi = {10.1145/2382577.2382579},
abstract = {Deemed “one of the top ten data mining mistakes”, leakage is the introduction of information about the data mining target that should not be legitimately available to mine from. In addition to our own industry experience with real-life projects, controversies around several major public data mining competitions held recently such as the INFORMS 2010 Data Mining Challenge and the IJCNN 2011 Social Network Challenge are evidence that this issue is as relevant today as it has ever been. While acknowledging the importance and prevalence of leakage in both synthetic competitions and real-life data mining projects, existing literature has largely left this idea unexplored. What little has been said turns out not to be broad enough to cover more complex cases of leakage, such as those where the classical independently and identically distributed (i.i.d.) assumption is violated, that have been recently documented. In our new approach, these cases and others are explained by explicitly defining modeling goals and analyzing the broader framework of the data mining problem. The resulting definition enables us to derive general methodology for dealing with the issue. We show that it is possible to avoid leakage with a simple specific approach to data management followed by what we call a learn-predict separation, and present several ways of detecting leakage when the modeler has no control over how the data have been collected. We also offer an alternative point of view on leakage that is based on causal graph modeling concepts.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {15},
numpages = {21},
keywords = {Data mining, leakage, predictive modeling}
}

@article{baesensRobROSERobustApproach2021,
  title = {{{robROSE}}: {{A}} Robust Approach for Dealing with Imbalanced Data in Fraud Detection},
  shorttitle = {{{robROSE}}},
  author = {Baesens, Bart and H{\"o}ppner, Sebastiaan and Ortner, Irene and Verdonck, Tim},
  year = {2021},
  month = sep,
  journal = {Statistical Methods \& Applications},
  volume = {30},
  number = {3},
  pages = {841--861},
  issn = {1613-981X},
  doi = {10.1007/s10260-021-00573-7},
  urldate = {2024-12-05},
  abstract = {A major challenge when trying to detect fraud is that the fraudulent activities form a minority class which make up a very small proportion of the data set. In most data sets, fraud occurs in typically less than \$\$0.5{\textbackslash}\%\$\$of the cases. Detecting fraud in such a highly imbalanced data set typically leads to predictions that favor the majority group, causing fraud to remain undetected. We discuss some popular oversampling techniques that solve the problem of imbalanced data by creating synthetic samples that mimic the minority class. A frequent problem when analyzing real data is the presence of anomalies or outliers. When such atypical observations are present in the data, most oversampling techniques are prone to create synthetic samples that distort the detection algorithm and spoil the resulting analysis. A useful tool for anomaly detection is robust statistics, which aims to find the outliers by first fitting the majority of the data and then flagging data observations that deviate from it. In this paper, we present a robust version of ROSE, called robROSE, which combines several promising approaches to cope simultaneously with the problem of imbalanced data and the presence of outliers. The proposed method achieves to enhance the presence of the fraud cases while ignoring anomalies. The good performance of our new sampling technique is illustrated on simulated and real data sets and it is shown that robROSE can provide better insight in the structure of the data. The source code of the robROSE algorithm is made freely available.},
  langid = {english},
  keywords = {Binary classification,Fraud analysis,Outliers,Oversampling,Skewed data},
  file = {C:\Users\abdusssboor\Zotero\storage\XGZ5KVGQ\Baesens et al. - 2021 - robROSE A robust approach for dealing with imbalanced data in fraud detection.pdf}
}
@INPROCEEDINGS{SyntheticPatki2016,
  author={Patki, Neha and Wedge, Roy and Veeramachaneni, Kalyan},
  booktitle={2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={The Synthetic Data Vault}, 
  year={2016},
  volume={},
  number={},
  pages={399-410},
  keywords={Data models;Databases;Computational modeling;Predictive models;Hidden Markov models;Numerical models;Synthetic data generation;crowd sourcing;data science;predictive modeling},
  doi={10.1109/DSAA.2016.49}}

@article{chawlaSMOTESyntheticMinority2002,
  title = {{{SMOTE}}: {{Synthetic Minority Over-sampling Technique}}},
  shorttitle = {{{SMOTE}}},
  author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
  year = {2002},
  month = jun,
  journal = {Journal of Artificial Intelligence Research},
  volume = {16},
  pages = {321--357},
  issn = {1076-9757},
  doi = {10.1613/jair.953},
  urldate = {2024-12-05},
  abstract = {An approach to the construction of classifiers from    imbalanced datasets is described. A dataset is imbalanced if the    classification categories are not approximately equally    represented. Often real-world data sets are predominately composed of    ``normal'' examples with only a small percentage of ``abnormal'' or    ``interesting'' examples. It is also the case that the cost of    misclassifying an abnormal (interesting) example as a normal example    is often much higher than the cost of the reverse    error. Under-sampling of the majority (normal) class has been proposed    as a good means of increasing the sensitivity of a classifier to the    minority class. This paper shows that a combination of our method of    over-sampling the minority (abnormal) class and under-sampling the    majority (normal) class can achieve better classifier performance (in    ROC space) than only under-sampling the majority class.  This paper    also shows that a combination of our method of over-sampling the    minority class and under-sampling the majority class can achieve    better classifier performance (in ROC space) than varying the loss    ratios in Ripper or class priors in Naive Bayes. Our method of    over-sampling the minority class involves creating synthetic minority    class examples.  Experiments are performed using C4.5, Ripper and a    Naive Bayes classifier. The method is evaluated using the area under    the Receiver Operating Characteristic curve (AUC) and the ROC convex    hull strategy.},
  copyright = {Copyright (c)},
  langid = {english},
  file = {C:\Users\abdusssboor\Zotero\storage\8N466JK4\Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf}
}


@article{garcia2009,
  author={He, Haibo and Garcia, Edwardo A.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Learning from Imbalanced Data}, 
  year={2009},
  volume={21},
  number={9},
  pages={1263-1284},
  keywords={Availability;Large-scale systems;Surveillance;Data security;IP networks;Finance;Data analysis;Decision making;Data engineering;Knowledge representation;Imbalanced learning;classification;sampling methods;cost-sensitive learning;kernel-based learning;active learning;assessment metrics.},
  doi={10.1109/TKDE.2008.239}}
  
 %related work 1
 @inproceedings{Patel2019, 
  author    = {D. K. Patel and S. Subudhi},
  title     = {Application of Extreme Learning Machine in Detecting Auto Insurance Fraud},
  booktitle = {2019 International Conference on Applied Machine Learning (ICAML)},
  year      = {2019},
  pages     = {78--81},
  address   = {Bhubaneswar, India},
  doi       = {10.1109/ICAML48257.2019.00023},
  publisher = {IEEE}
}
%related work 2
@inproceedings{Harjai2019, 
  author    = {S. Harjai and S. K. Khatri and G. Singh},
  title     = {Detecting Fraudulent Insurance Claims Using Random Forests and Synthetic Minority Oversampling Technique},
  booktitle = {2019 4th International Conference on Information Systems and Computer Networks (ISCON)},
  year      = {2019},
  pages     = {123--128},
  address   = {Mathura, India},
  doi       = {10.1109/ISCON47742.2019.9036162},
  publisher = {IEEE}
}
%related work 3
@inproceedings{Salmi2022, 
  author    = {M. Salmi and D. Atif},
  title     = {Using a data mining approach to detect automobile insurance fraud},
  booktitle = {Proceedings of the 13th International Conference on Soft Computing and Pattern Recognition (SoCPaR)},
  year      = {2022},
  pages     = {55--66},
  publisher = {Springer},
  address   = {Cham, Switzerland},
  doi       = {10.1007/978-3-030-96302-6_5},
  url       = {https://doi.org/10.1007/978-3-030-96302-6_5}
}
%related work 4
@inproceedings{Padhi2020, 
  author    = {S. Padhi and S. Panigrahi},
  title     = {Use of data mining techniques for data balancing and fraud detection in automobile insurance claims},
  booktitle = {Proceedings of the International Conference on Computational Intelligence and Data Science (ICCIDS 2019)},
  volume    = {1044},
  year      = {2020},
  pages     = {259--268},
  publisher = {Springer},
  address   = {Singapore},
  doi       = {10.1007/978-981-15-1084-7_22},
  url       = {http://link.springer.com/10.1007/978-981-15-1084-7_22}
}
 %related work 5
@inproceedings{Wongpanti2024,
  author    = {R. Wongpanti and S. Vittayakorn},
  title     = {Enhancing Auto Insurance Fraud Detection Using Convolutional Neural Networks},
  booktitle = {2024 21st International Joint Conference on Computer Science and Software Engineering (JCSSE)},
  year      = {2024},
  pages     = {294--301},
  address   = {Phuket, Thailand},
  doi       = {10.1109/JCSSE61278.2024.10613702},
  publisher = {IEEE}
}
@article{Clarke1990THECO,
  title={THE CONTROL OF INSURANCE FRAUDA Comparative View},
  author={Michael Clarke},
  journal={British Journal of Criminology},
  year={1990},
  volume={30},
  pages={1-23},
  url={https://api.semanticscholar.org/CorpusID:147055879}
}

@article{viaeneInsuranceFraudIssues2004,
  title = {Insurance {{Fraud}}: {{Issues}} and {{Challenges}}},
  shorttitle = {Insurance {{Fraud}}},
  author = {Viaene, Stijn and Dedene, Guido},
  year = {2004},
  month = apr,
  journal = {The Geneva Papers on Risk and Insurance - Issues and Practice},
  volume = {29},
  number = {2},
  pages = {313--333},
  issn = {1468-0440},
  doi = {10.1111/j.1468-0440.2004.00290.x},
  urldate = {2024-12-05},
  abstract = {This article is devoted to the phenomenon of insurance fraud. We start by surveying the various forms of insurance fraud as well as its extent and cost. We proceed to analyse the problem as the product of motivation and opportunity, and address the complexities of fraud control. Finally, we provide a high-level overview of current anti-fraud activity.},
  langid = {english},
  file = {C:\Users\abdusssboor\Zotero\storage\E37W3EX4\Viaene and Dedene - 2004 - Insurance Fraud Issues and Challenges.pdf}
}

@article{belhadjiModelDetectionInsurance2000,
  title = {A {{Model}} for the {{Detection}} of {{Insurance Fraud}}},
  author = {Belhadji, El Bachir and Dionne, Georges and Tarkhani, Faouzi},
  year = {2000},
  journal = {The Geneva Papers on Risk and Insurance. Issues and Practice},
  volume = {25},
  number = {4},
  eprint = {41952549},
  eprinttype = {jstor},
  pages = {517--538},
  publisher = {Palgrave Macmillan Journals},
  issn = {1018-5895},
  urldate = {2024-12-05},
  abstract = {The aim of this article is to develop a model to aid insurance companies in their decisionmaking and to ensure that they are better equipped to fight fraud. This tool is based on the systematic use of fraud indicators. We first propose a procedure to isolate the indicators which are most significant in predicting the probability that a claim may be fraudulent. We applied the procedure to data collected in the Dionne-Belhadji study (1996). The model allowed us to observe that 23 of the 54 indicators used were significant in predicting the probability of fraud. Our study also discusses the model's accuracy and detection capability. The detection rates obtained by the adjusters who participated in the study constitute the reference point of this discussion. As shown in the Caron-Dionne (1998), there is the possibility that these rates underestimate the true level of fraud.}
}
@article{BERMUDEZ,
title = {A Bayesian dichotomous model with asymmetric link for fraud in insurance},
journal = {Insurance: Mathematics and Economics},
volume = {42},
number = {2},
pages = {779-786},
year = {2008},
issn = {0167-6687},
doi = {https://doi.org/10.1016/j.insmatheco.2007.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167668707000947},
author = {Ll. Bermúdez and J.M. Pérez and M. Ayuso and E. Gómez and F.J. Vázquez},
keywords = {IB40, IM20, Bayesian statistics, Logit model, Gibbs sampling, Automobile insurance, Fraud},
abstract = {Standard binary models have been developed to describe the behavior of consumers when they are faced with two choices. The classical logit model presents the feature of the symmetric link function. However, symmetric links do not provide good fits for data where one response is much more frequent than the other (as it happens in the insurance fraud context). In this paper, we use an asymmetric or skewed logit link, proposed by Chen et al. [Chen, M., Dey, D., Shao, Q., 1999. A new skewed link model for dichotomous quantal response data. J. Amer. Statist. Assoc. 94 (448), 1172–1186], to fit a fraud database from the Spanish insurance market. Bayesian analysis of this model is developed by using data augmentation and Gibbs sampling. The results show that the use of an asymmetric link notably improves the percentage of cases that are correctly classified after the model estimation.}
}

@misc{kilroyInsuranceFraudStatistics2024,
  title = {Insurance {{Fraud Statistics}} 2024},
  author = {Kilroy, Ashley},
  year = {2024},
  month = mar,
  journal = {Forbes Advisor},
  urldate = {2024-12-05},
  abstract = {Insurance fraud remains a widespread issue in the U.S., costing businesses, insurers and consumers billions each year. It entails intentional deception throughout the insurance process, from policy purchase to claims settlement. Health care insurance fraud, particularly Medicare and Medicaid fraud,},
  chapter = {Insurance},
  howpublished = {https://www.forbes.com/advisor/insurance/fraud-statistics/},
  langid = {american},
  file = {C:\Users\abdusssboor\Zotero\storage\TY496JDD\fraud-statistics.html}
}
@article{GOMEZDENIZ,
title = {A generalization of the credibility theory obtained by using the weighted balanced loss function},
journal = {Insurance: Mathematics and Economics},
volume = {42},
number = {2},
pages = {850-854},
year = {2008},
issn = {0167-6687},
doi = {https://doi.org/10.1016/j.insmatheco.2007.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S016766870700100X},
author = {E. Gómez-Déniz},
keywords = {IM30, Loss function, Credibility premium, Net, Esscher, Distribution free approach},
abstract = {In this paper an alternative to the usual credibility premium that arises for weighted balanced loss function is considered. This is a generalized loss function which includes as a particular case the weighted quadratic loss function traditionally used in actuarial science. From this function credibility premiums under appropriate likelihood and priors can be derived. By using weighted balanced loss function we obtain, first, generalized credibility premiums that contain as particular cases other credibility premiums in the literature and second, a generalization of the well-known distribution free approach in [Bühlmann, H., 1967. Experience rating and credibility. Astin Bull. 4 (3), 199–207].}
}
@article{AndreasRp
author={Andersson,Jonas and Olden,Andreas and Rusina,Aija},
year={2020},
month={-12-31},
title={Fraud detection by a multinomial model: Separating honesty from unobserved fraud},
abstract={In this paper we investigate the EM-estimator of the model by Caudill et al. (2005). The purpose of the model is to identify items, e.g. individuals or companies, that are wrongly classified as honest; an example of this is the detection of tax evasion. Normally, we observe two groups of items, labeled fradulent and honest, but suspect that many of the observationally honest items are, in fact, fraudulent. The items observed as honest are therefore divided into two unobserved groups, honestH, representing the truly honest, and honestF, representing the items that are observed as honest, but that are actually fraudulent. By using a multinomial logit model and assuming commonality between the observed fradulent and the unobserved honestF, Caudill et al. (2005) present a method that uses the EM-algorithm to separate them. By means of a Monte Carlo study, we investigate how well the method performs, and under what circumstances. We also study how well boostrapped standard errors estimates the standard deviation of the parameter estimators.},
}

@article{XuRp,
author={Xu,Lei and Skoularidou,Maria and Cuesta-Infante,Alfredo and Veeramachaneni,Kalyan},
title={Modeling Tabular Data using Conditional GAN},
abstract={Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design CTGAN, which uses a conditional generator to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. CTGAN outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.},
}

@incollection{fernandezPerformanceMeasures2018a,
  title = {Performance {{Measures}}},
  booktitle = {Learning from {{Imbalanced Data Sets}}},
  author = {Fern{\'a}ndez, Alberto and Garc{\'i}a, Salvador and Galar, Mikel and Prati, Ronaldo C. and Krawczyk, Bartosz and Herrera, Francisco},
  editor = {Fern{\'a}ndez, Alberto and Garc{\'i}a, Salvador and Galar, Mikel and Prati, Ronaldo C. and Krawczyk, Bartosz and Herrera, Francisco},
  year = {2018},
  pages = {47--61},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-98074-4_3},
  urldate = {2024-12-05},
  abstract = {Analyzing the performance of learning algorithms under presence of class imbalance is a difficult task. For some widely-used measures, such as accuracy, the prevalence of more frequent classes may mask a poor classification performance in infrequent classes. To alleviate this problem, the choice of suitable measures is of fundamental importance. This chapter presents some performance measures that can be used to evaluate classification performance under presence of class imbalance, highlighting their advantages and drawbacks. With aims at presenting this content, the chapter is organized as follows: First, Sect. 3.1 sets the background on the evaluation procedure. Then, Sect. 3.2 presents performance measures for crisp, nominal predictions. Section 3.3 discuss evaluation methods for scoring classifiers. Finally, Sect. 3.4 discuss probabilistic evaluation, and Sect. 3.5 concludes the chapter.},
  isbn = {978-3-319-98074-4},
  langid = {english},
  file = {C:\Users\abdusssboor\Zotero\storage\TV8WHTMZ\Fernández et al. - 2018 - Performance Measures.pdf}
}

@article{schrijverAutomobileInsuranceFraud2024,
  title = {Automobile Insurance Fraud Detection Using Data Mining: {{A}} Systematic Literature Review},
  shorttitle = {Automobile Insurance Fraud Detection Using Data Mining},
  author = {Schrijver, Gilian and Sarmah, Dipti K. and {El-hajj}, Mohammed},
  year = {2024},
  month = mar,
  journal = {Intelligent Systems with Applications},
  volume = {21},
  pages = {200340},
  issn = {2667-3053},
  doi = {10.1016/j.iswa.2024.200340},
  urldate = {2024-11-03},
  abstract = {Insurance is a pivotal element in modern society, but insurers face a persistent challenge from fraudulent behaviour performed by policyholders. This behaviour could be detrimental to both insurance companies and their honest customers, but the intricate nature of insurance fraud severely complicates its efficient, automated detection. This study surveys fifty recent publications on automobile insurance fraud detection, published between January 2019 and March 2023, and presents both the most commonly used data sets and methods for resampling and detection, as well as interesting, novel approaches. The study adopts the highly-cited Systematic Literature Review (SLR) methodology for software engineering research proposed by Kitchenham and Charters and collected studies from four online databases. The findings indicate limited public availability of automobile insurance fraud data sets. In terms of detection methods, the prevailing approach involves supervised machine learning methods that utilise structured, intrinsic features of claims or policies and that lack consideration of an example-dependent cost of misclassification. However, alternative techniques are also explored, including the use of graph-based methods, unstructured textual data, and cost-sensitive classifiers. The most common resampling approach was found to be oversampling. This SLR has identified commonly used methods in recent automobile insurance fraud detection research, and interesting directions for future research. It adds value over a related review by also including studies published from 2021 onward, and by detailing the used methodology. Limitations of this SLR include its restriction to a small number of considered publication years and limited validation of choices made during the process.},
  keywords = {Automobile insurance,Data mining,Fraud detection,Insurance fraud,Machine learning,Systematic literature review},
  file = {C\:\\Users\\abdusssboor\\Zotero\\storage\\QE2DXYGF\\Schrijver et al. - 2024 - Automobile insurance fraud detection using data mining A systematic literature review.pdf;C\:\\Users\\abdusssboor\\Zotero\\storage\\AN3UHPYR\\S2667305324000164.html}
}

@inproceedings{aiemsuwanNovelHybridMethod2024,
  title = {A {{Novel Hybrid Method}} for {{Imbalanced Automobile Insurance Fraud Detection}}},
  booktitle = {2024 16th {{International Conference}} on {{Knowledge}} and {{Smart Technology}} ({{KST}})},
  author = {Aiemsuwan, Phannana and Srikamdee, Supawadee},
  year = {2024},
  month = feb,
  pages = {12--17},
  issn = {2473-764X},
  doi = {10.1109/KST61284.2024.10499643},
  urldate = {2024-11-03},
  abstract = {This paper addresses fraud detection in the automobile insurance sector, which experiences the highest rate of fraudulent claims in the industry. The study proposes a methodology that combines resampling techniques and backward elimination for feature selection to tackle data imbalance. This approach significantly improves the accuracy of machine learning models in identifying fraudulent activities. The research conducts a comprehensive comparison of seven major machine learning algorithms: Logistic Regression, Decision Tree, Random Forest, k-nearest Neighbors, Naive Bayes, XGBoost, and Support Vector Machine. These models are evaluated based on precision, recall, and F1 score across three different datasets to determine their effectiveness in fraud detection. The findings reveal that the Random Forest algorithm is the most efficient, consistently achieving F1 scores above 0.92. This highlights the vital role of machine learning in detecting and preventing fraud in the automotive insurance industry.},
  keywords = {backward elimination,Data models,Feature extraction,fraud detection,Industries,Insurance,Machine learning algorithms,Predictive models,resampling method,Support vector machines},
  file = {C\:\\Users\\abdusssboor\\Zotero\\storage\\SYU8NY2N\\Aiemsuwan and Srikamdee - 2024 - A Novel Hybrid Method for Imbalanced Automobile Insurance Fraud Detection.pdf;C\:\\Users\\abdusssboor\\Zotero\\storage\\Q4N3LHZ7\\10499643.html}
}

@inproceedings{owolabiAutoInsuranceFraudDetection2024,
  title = {Auto-{{Insurance Fraud Detection Using Machine Learning Classification Models}}},
  booktitle = {Proceedings of {{Eighth International Congress}} on {{Information}} and {{Communication Technology}}},
  author = {Owolabi, Toluwalope and Shahra, Essa Q. and Basurra, Shadi},
  editor = {Yang, Xin-She and Sherratt, R. Simon and Dey, Nilanjan and Joshi, Amit},
  year = {2024},
  pages = {503--513},
  publisher = {Springer Nature},
  address = {Singapore},
  doi = {10.1007/978-981-99-3043-2_39},
  abstract = {This work explored six machine learning algorithms: Extreme Gradient Boosting (XGBoost), Logistic Regression, Random Forest, Decision tree, Support Vector Machine (SVM), and Na{\"i}ve Bayes to determine the best algorithm for detecting insurance fraud. The following were used to evaluate the six models: Confusion matrix, Accuracy, Precision, Recall, and F1-measure. The result showed that Random Forest outperformed the others in terms of accuracy. Extreme Gradient Boosting (Xgboost) had the highest precision and F1-measure scores, while the Decision Tree had the highest Recall score. Although two methods (Analysis of Variance (ANOVA) and Random Forest Classifier) were compared to determine the best feature selection, the significant features were selected using the Random Forest classifier because of the many benefits of using this method. The results of this study will be beneficial to insurance companies, stakeholders and policyholders.},
  isbn = {978-981-99-3043-2},
  langid = {english},
  keywords = {Classification,Fraud detection,Machine learning,Random forest},
  file = {C:\Users\abdusssboor\Zotero\storage\DKWQCAH5\Owolabi et al. - 2024 - Auto-Insurance Fraud Detection Using Machine Learning Classification Models.pdf}
}

@inproceedings{
    SDV,
    title={The Synthetic data vault},
    author={Patki, Neha and Wedge, Roy and Veeramachaneni, Kalyan},
    booktitle={IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
    year={2016},
    pages={399-410},
    doi={10.1109/DSAA.2016.49},
    month={Oct}
}

@article{bergstraMakingScienceModel,
  title = {Making a {{Science}} of {{Model Search}}: {{Hyperparameter Optimization}} in {{Hundreds}} of {{Dimensions}} for {{Vision Architectures}}},
  author = {Bergstra, J and Yamins, D and Cox, D D},
  abstract = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method's full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned.},
  langid = {english},
  file = {C:\Users\abdusssboor\Zotero\storage\T4WH5WC4\Bergstra et al. - Making a Science of Model Search Hyperparameter Optimization in Hundreds of Dimensions for Vision A.pdf}
}


